{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Description about Import\n",
        "1. os: The os module in Python provides a way of using operating system-dependent functionality. In this context, it might be used for operations related to file and directory manipulation, such as checking file existence or listing directory contents.\n",
        "\n",
        "2. cv2 (OpenCV): OpenCV (Open Source Computer Vision Library) is a library of programming functions mainly aimed at real-time computer vision. It's commonly used for image processing tasks such as reading, writing, and manipulating images, as well as various computer vision algorithms.\n",
        "\n",
        "3. numpy (alias np): NumPy is a powerful Python library for numerical computing. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays efficiently. In this context, it might be used for numerical operations on image data.\n",
        "\n",
        "4. matplotlib.pyplot (alias plt): Matplotlib is a plotting library for Python that provides a MATLAB-like interface. The pyplot module provides a convenient way to create plots and visualizations. It can be used here to display images or visualize data.\n",
        "\n",
        "5. tensorflow (alias tf): TensorFlow is an open-source machine learning library developed by Google. It provides a comprehensive ecosystem of tools and libraries for building and deploying machine learning models, including neural networks. In this context, it might be used for building and training neural networks for tasks such as image classification or object detection."
      ],
      "metadata": {
        "id": "sSRSOvNgaBmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW774uY3Rngg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data Set"
      ],
      "metadata": {
        "id": "EXMxsIeda04q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist"
      ],
      "metadata": {
        "id": "Ryl2OhrlSVzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpSXXIWzSlYo",
        "outputId": "f2e1d18b-9eeb-4ac2-cc13-a5db4ffa74df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "omg5s8sMa737"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### <b> Normalization </b>\n",
        " Normalization is a common preprocessing technique used in machine learning to scale the input features to a similar range. This ensures that each feature contributes approximately proportionately to the learning process and prevents any particular feature from dominating due to its larger magnitude.\n",
        "\n",
        "<b> 'tf.keras.utils.normalize'</b> : This function normalizes the input data along the specified axis. In this case, axis=1 indicates that normalization is performed along the feature axis, i.e., each feature vector is normalized independently. The normalization is typically done by dividing each feature vector by its Euclidean norm (L2-norm), resulting in a unit norm for each sample.\n",
        "\n",
        " <b>Reasons for Normalization:</b>\n",
        "\n",
        "- Improved Convergence: Normalizing the input data can help the optimization algorithm converge faster during training. When features are on different scales, the learning rate may need to be adjusted to accommodate the varying magnitudes, leading to slower convergence.\n",
        "\n",
        "- Stability: Normalization can improve the numerical stability of the training process, especially for algorithms sensitive to the scale of input features. It prevents issues such as vanishing or exploding gradients.\n",
        "\n",
        "- Generalization: Normalization can aid in better generalization of the model by preventing overfitting. It helps the model focus on learning meaningful patterns rather than being influenced by the scale of the input features.\n",
        "\n",
        "- Interpretability: Normalized features are often easier to interpret, as they are on a similar scale, making comparisons between features more straightforward."
      ],
      "metadata": {
        "id": "EUwnuc4-bBtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf.keras.utils.normalize(X_train, axis = 1)\n",
        "X_test = tf.keras.utils.normalize(X_test, axis = 1)"
      ],
      "metadata": {
        "id": "igJV_mOdS9Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Model"
      ],
      "metadata": {
        "id": "7nkUKL0Vb_fN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a neural network model using TensorFlow's Keras API.\n",
        "\n",
        "<b>tf.keras.models.Sequential()</b>: This creates a sequential model, which is a linear stack of layers. In this type of model, each layer has exactly one input tensor and one output tensor. The sequential model is suitable for most deep learning tasks, where the data flows sequentially through the layers.\n",
        "\n",
        "<b>model.add(tf.keras.layers.Flatten(input_shape=(28,28)))</b>: The Flatten layer is the first layer of the model. It transforms the input data, which in this case is 2D (28x28) images, into a 1D array (or vector) of size 784 (28*28). This flattening step is necessary because the subsequent layers, such as densely connected layers, require a 1D input.\n",
        "\n",
        "<b>model.add(tf.keras.layers.Dense(128, activation='relu'))</b>: This adds a fully connected (dense) layer with 128 neurons and ReLU (Rectified Linear Unit) activation function. Dense layers are used to capture complex patterns in the data by connecting each neuron to every neuron in the previous layer. The ReLU activation function introduces non-linearity to the model, allowing it to learn and represent non-linear relationships in the data.\n",
        "\n",
        "<b>model.add(tf.keras.layers.Dense(128, activation='relu'))</b>: Another fully connected layer with 128 neurons and ReLU activation function is added. This additional layer can help the model learn more complex representations from the data and potentially improve its performance.\n",
        "\n",
        "<b>model.add(tf.keras.layers.Dense(10, activation='softmax'))</b>: The final layer is a fully connected layer with 10 neurons, corresponding to the number of classes in the classification task (assuming this is a classification problem with 10 classes). The softmax activation function is used in this layer to output probabilities for each class. Softmax ensures that the output values sum up to 1, making them interpretable as probabilities. The predicted class is typically the one with the highest probability.\n",
        "\n",
        "<b>Reasons for this architecture:</b>\n",
        "\n",
        "- Input Layer: The Flatten layer converts the 2D image data into a format suitable for the subsequent dense layers.\n",
        "- Hidden Layers: The two Dense layers with ReLU activation introduce non-linearity and enable the model to learn complex patterns in the data.\n",
        "- Output Layer: The final Dense layer with softmax activation produces the probability distribution over the output classes, allowing the model to make predictions."
      ],
      "metadata": {
        "id": "NFZnbsoecUOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28,28)))\n",
        "model.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
        "model.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "-syqdhWITQnJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the Model"
      ],
      "metadata": {
        "id": "iGgoWCGUc2uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The compile method configures the model for training.\n",
        "\n",
        "<b>optimizer='adam'</b>: This specifies the optimizer to be used during training. Adam (short for Adaptive Moment Estimation) is a popular optimization algorithm that combines ideas from RMSProp and Momentum. It's well-suited for a wide range of deep learning tasks due to its adaptive learning rate and momentum properties. Adam adapts the learning rate for each parameter individually, which can lead to faster convergence and better performance compared to traditional optimization algorithms.\n",
        "\n",
        "<b>loss='sparse_categorical_crossentropy'</b>: This specifies the loss function to be used during training. Cross-entropy loss is commonly used in classification tasks to measure the difference between the predicted probability distribution and the true distribution of the labels. In this case, sparse_categorical_crossentropy is used because the labels are integers (sparse representation) and not one-hot encoded. This loss function calculates the cross-entropy between the true labels and the predicted probabilities.\n",
        "\n",
        "<b>metrics='accuracy'</b>: This specifies the evaluation metric to be used during training and testing. Accuracy is a common metric used in classification tasks to measure the proportion of correctly classified samples. It calculates the percentage of samples for which the predicted class matches the true class. Monitoring accuracy during training provides insights into how well the model is learning to classify the data correctly.\n",
        "\n",
        "<b>Reasons for these choices:</b>\n",
        "\n",
        "Adam Optimizer: Adam is a popular choice for optimization due to its adaptive learning rate, which helps to adjust the learning rates for different parameters individually. This adaptiveness can lead to faster convergence and better generalization.\n",
        "\n",
        "Sparse Categorical Crossentropy Loss: Since the labels are integers (not one-hot encoded), sparse_categorical_crossentropy is appropriate for this classification task. It computes the cross-entropy loss between the true labels and the predicted probabilities, providing a measure of how well the model's predictions match the true distribution of the labels.\n",
        "\n",
        "Accuracy Metric: Accuracy is a straightforward and intuitive metric for classification tasks. It provides a clear measure of the model's performance in terms of correctly classified samples. Monitoring accuracy during training helps to assess the progress of the model and its ability to classify the data correctly."
      ],
      "metadata": {
        "id": "djbE1wKydPck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')"
      ],
      "metadata": {
        "id": "SeTbrSA4UbNS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An epoch is one complete pass through the entire training dataset. The epochs parameter specifies the number of times the training algorithm will iterate over the entire dataset during training. In this case, epochs = 3 indicates that the training process will iterate over the dataset three times.\n",
        "\n",
        "<b>Reasons for these choices:</b>\n",
        "\n",
        "Training Data: The fit method requires both input features (X_train) and corresponding target labels (y_train) to train the model. This allows the model to learn the mapping between the input data and the target labels, enabling it to make predictions on unseen data.\n",
        "\n",
        "Number of Epochs: Training a neural network involves adjusting the model's parameters (weights and biases) iteratively to minimize the loss function on the training data. Increasing the number of epochs allows the model to see the training data multiple times, potentially improving its ability to learn complex patterns in the data and converge to a better solution. However, using too many epochs can lead to overfitting, where the model memorizes the training data instead of learning generalizable patterns. Choosing an appropriate number of epochs involves balancing the trade-off between training time and model performance on unseen data.\n"
      ],
      "metadata": {
        "id": "rlkyTNVQdopq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs = 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKlynJlGVu_6",
        "outputId": "18e9477d-c337-4b9d-dbac-76494c203297"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1062 - accuracy: 0.9673\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0711 - accuracy: 0.9773\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0534 - accuracy: 0.9830\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f19e6177190>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('handwritten.model')"
      ],
      "metadata": {
        "id": "K7havQuNV12Y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('handwritten.model')"
      ],
      "metadata": {
        "id": "0hSzu075V7VY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubMTL3MTWb2g",
        "outputId": "b41345c5-e64c-439f-e907-1782d19f11f4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0883 - accuracy: 0.9716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvhH_mgdWmxz",
        "outputId": "8f05f6b2-5787-4b9d-9c60-75330a1a9c24"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08826439827680588\n",
            "0.9715999960899353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function can be used to evaluate reall life hand written images to evaluate the model."
      ],
      "metadata": {
        "id": "plmIBLMYf2v_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_number = 1\n",
        "while os.path.isfile(f\"digits/digit{image_number}.png\"):\n",
        "  try:\n",
        "    img = cv2.imread(f\"digits/digit{image_number}.png\")[:,:,0]\n",
        "    img - np.invert(np.array([img]))\n",
        "    prediction = model.predict(img)\n",
        "    print(\"The digit is probably a {np.argmax(prediciton)}\")\n",
        "    plt.imshow(img[0], cmap=plt.cm.binary)\n",
        "  except:\n",
        "    print(\"Error\")\n",
        "  finally:\n",
        "    image_number+=1"
      ],
      "metadata": {
        "id": "rlSCFK61WsOy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra\n",
        "\n",
        "### Layers in Neural Networks\n",
        "\n",
        "<b> 1. Input Layer: </b>\n",
        "\n",
        "- The input layer is the first layer in the neural network.\n",
        "- It receives the input data and passes it on to the next layer.\n",
        "- It doesn't perform any computations and simply serves as the entry point for the data.\n",
        "- In TensorFlow/Keras, the input layer is often implicitly defined when specifying the input shape in the first hidden layer.\n",
        "\n",
        "<b> 2. Hidden Layers:</b>\n",
        "\n",
        "-Hidden layers are the intermediate layers between the input and output layers.\n",
        "- Each hidden layer consists of multiple neurons (nodes) that perform computations on the input data.\n",
        "- The number of hidden layers and neurons in each hidden layer can vary based on the complexity of the problem and the desired model architecture.\n",
        "Each neuron in a hidden layer applies a weighted sum of inputs, followed by an activation function to introduce non-linearity.\n",
        "- Common activation functions used in hidden layers include ReLU (Rectified Linear Unit), sigmoid, tanh (Hyperbolic Tangent), etc.\n",
        "\n",
        "<b> 3. Output Layer:</b>\n",
        "\n",
        "- The output layer is the final layer in the neural network.\n",
        "- It produces the output predictions of the model based on the computations performed in the hidden layers.\n",
        "- The number of neurons in the output layer depends on the nature of the problem:\n",
        "For binary classification problems, a single neuron with a sigmoid activation function is often used to output a probability score between 0 and 1.\n",
        "For multi-class classification problems, the number of neurons in the output layer corresponds to the number of classes, with each neuron representing the probability of belonging to a particular class (usually softmax activation is used).\n",
        "For regression problems, the output layer typically has a single neuron without any activation function, or a specific activation function based on the range of target values.\n",
        "\n",
        "### Optimizers\n",
        "\n",
        "1. Stochastic Gradient Descent (SGD):\n",
        "This is the most basic optimization algorithm.\n",
        "It updates the weights in the opposite direction of the gradient of the loss function with respect to the weights.\n",
        "It can have trouble navigating complex loss surfaces and finding the global minimum.\n",
        "\n",
        "2. Adam (Adaptive Moment Estimation):\n",
        "Adam combines the ideas of Momentum and RMSProp.\n",
        "It adapts the learning rates for each parameter based on estimates of the first and second moments of the gradients.\n",
        "It is computationally efficient and widely used in practice.\n",
        "\n",
        "3. RMSProp (Root Mean Square Propagation):\n",
        "RMSProp adjusts the learning rates for each parameter based on the average of the recent magnitudes of the gradients for that parameter.\n",
        "It divides the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight.\n",
        "It helps to alleviate the diminishing learning rates problem in Adagrad.\n",
        "\n",
        "4. Adagrad (Adaptive Gradient Algorithm):\n",
        "Adagrad adapts the learning rates for each parameter based on the historical gradients for that parameter.\n",
        "It scales the learning rate of each weight by the inverse of the square root of the sum of the squares of the historical gradients for that weight.\n",
        "It performs well for sparse data.\n",
        "\n",
        "5. Adadelta:\n",
        "Adadelta is an extension of Adagrad that tries to address its diminishing learning rates problem.\n",
        "It uses a more sophisticated update rule that eliminates the need for an explicit learning rate.\n",
        "It adapts the learning rates based on a moving window of gradient updates.\n",
        "\n",
        "6. Adamax:\n",
        "Adamax is a variant of Adam that uses the infinity norm of the gradients instead of the L2 norm.\n",
        "It is more robust to the choice of initial learning rate and less affected by vanishing gradients compared to Adam.\n",
        "\n",
        "7. Nadam (Nesterov-accelerated Adaptive Moment Estimation):\n",
        "Nadam is a variant of Adam that incorporates the Nesterov accelerated gradient (NAG) method.\n",
        "It combines the benefits of Adam and NAG to improve convergence rates and stability.\n",
        "\n",
        "### Loses\n",
        "\n",
        "1. Mean Squared Error (MSE):\n",
        "MSE is used for regression tasks.\n",
        "It calculates the average of the squared differences between the predicted and actual values.\n",
        "MSE penalizes larger errors more heavily than smaller errors.\n",
        "\n",
        "2. Binary Cross-Entropy Loss (Binary Crossentropy):\n",
        "Binary cross-entropy loss is used for binary classification tasks.\n",
        "It measures the difference between the probability distributions of the true labels and the predicted probabilities.\n",
        "It is commonly used with sigmoid activation in the output layer.\n",
        "\n",
        "3. Categorical Cross-Entropy Loss (Categorical Crossentropy):\n",
        "Categorical cross-entropy loss is used for multi-class classification tasks.\n",
        "It measures the difference between the true label distribution and the predicted probabilities.\n",
        "It is commonly used with softmax activation in the output layer.\n",
        "\n",
        "4. Sparse Categorical Cross-Entropy Loss (Sparse Categorical Crossentropy):\n",
        "Sparse categorical cross-entropy loss is similar to categorical cross-entropy but is used when the true labels are integers (not one-hot encoded).\n",
        "It is commonly used for multi-class classification tasks where the labels are represented as integers.\n",
        "\n",
        "5. Binary Hinge Loss (Hinge):\n",
        "Hinge loss is used for binary classification tasks, especially in support vector machines (SVMs).\n",
        "It penalizes misclassifications linearly and encourages correct classification with a margin.\n",
        "It is commonly used in SVMs and in neural networks with linear activation in the output layer.\n",
        "\n",
        "6. Kullback-Leibler Divergence (KLD):\n",
        "KLD is a measure of how one probability distribution diverges from a second, expected probability distribution.\n",
        "It is commonly used in variational autoencoders (VAEs) as a regularization term to encourage the learned distribution to match a predefined distribution."
      ],
      "metadata": {
        "id": "kA3VkxnAgvoC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EZRYRQtagsx-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}